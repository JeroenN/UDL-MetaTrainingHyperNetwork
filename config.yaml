training:
  batch_size_outerloop: 256
  batch_size_innerloop: 128
  batch_size_vae: 256

  epochs_hyper: 1000
  epochs_vae: 2

  lr_hyper: 1e-4
  lr_vae: 1e-4

  log_interval: 2
  save_path: "hypernet_checkpoint.pth"

meta:
  steps_innerloop: 1
  steps_outerloop: 10

data:
  image_width_height: 28

vae:
  vae_head_dim: 10
  n_samples_conditioning: 100
  retrain_vae: false
  vae_description: "_head_10"

model:
  cluster_using_guassians: true
  use_contrastive_loss: true
  contrastive_temp: 0.07

hypernet:
  embed_dim: 0
  head_hidden: 256
  use_bias: true

target_net:
  # If cluster_using_guassians=true, input_dim is 2*vae_head_dim (mu + logvar)
  # Else, input_dim is image_width_height^2 (flattened image)
  hidden_layers: [400, 200]
  num_classes: 10
